{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Top-Down Approach \n",
    "\n",
    "The coursebook is part of the **Data Analytics Specialization** offered by [Algoritma](https://algorit.ma). It takes a more accessible approach compared to Algoritma's core educational products, by getting participants to overcome the \"how\" barrier first, rather than a detailed breakdown of the \"why\". \n",
    "\n",
    "This translates to an overall easier learning curve, one where the reader is prompted to write short snippets of code in frequent intervals, before being offered an explanation on the underlying theoretical frameworks. Instead of mastering the syntactic design of the Python programming language, then moving into data structures, and then the `pandas` library, and then the mathematical details in an imputation algorithm, and its code implementation; we would do the opposite: Implement the imputation, then a succinct explanation of why it works and applicational considerations (what to look out for, what are assumptions it made, when _not_ to use it etc).\n",
    "\n",
    "## Training Objectives\n",
    "\n",
    "This coursebook is intended for participants who have completed the preceding courses offered in the **Data Analytics Developer** Specialization. This is the second course, **Exploratory Data Analysis**\n",
    "\n",
    "The coursebook focuses on:\n",
    "- Why and What: Exploratory Data Analysis\n",
    "- Date Time objects\n",
    "- Categorical data types\n",
    "- Cross Tabulation and Pivot Table\n",
    "- Treating Duplicates and Missing Values \n",
    "\n",
    "At the end of this course is a Learn-by-Building section, where you are expected to apply all that you've learned on a new dataset, and attempt the given questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration\n",
    "\n",
    "About 60 years ago, John Tukey defined data analysis as the \"procedures for analyzing data, techniques for interpreting the results of such procedures ... and all the machinery of mathematical statistics which apply to analyzing data\". His championing of EDA encouraged the development of statsitical computing packages, especially S at Bell Labs (which later inspired R).\n",
    "\n",
    "He wrote a book titled _Exploratory Data Analysis_ arguing that too much emphasis in statistics was placed on hypothesis testing (confirmatory data analysis) while not enough was placed on the discovery of the unexpected. \n",
    "\n",
    "> Exploratory data analysis isolates patterns and features of the data and reveals these forcefully to the analyst.\n",
    "\n",
    "This course aims to present a selection of EDA techniques -- some developed by John Tukey himself -- but with a special emphasis on its application to modern business analytics.\n",
    "\n",
    "In the previous course, we've got our hands on a few common techniques:\n",
    "\n",
    "- `.head()` and `.tail()`\n",
    "- `.describe()`\n",
    "- `.shape` and `.size`\n",
    "- `.axes`\n",
    "- `.dtypes`\n",
    "\n",
    "In the following chapters, we'll expand our EDA toolset with the following additions:  \n",
    "\n",
    "- Tables\n",
    "- Cross-Tables and Aggregates\n",
    "- Using `aggfunc` for aggregate functions\n",
    "- Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(pd.__version__)\n",
    "\n",
    "# pandas output display setup\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) \n",
    "pd.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Datetime\n",
    "\n",
    "Given the program's special emphasis on business-driven analytics, one data type of particular interest to us is the `datetime`. In the first part of this coursebook, we've seen an example of `datetime` in the section introducing data types (`employees.joined`).\n",
    "\n",
    "A large portion of data science work performed by business executives involve time series and/or dates (think about the kind of data science work done by computer vision researchers, and compare that to the work done by credit rating analysts or marketing executives and this special relationship between business and datetime data becomes apparent), so adding a level of familiarity with this format will serve you well in the long run. \n",
    "\n",
    "As a start, let's read our data,`household.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id            int64\n",
       "receipts_item_id      int64\n",
       "purchase_time        object\n",
       "category             object\n",
       "sub_category         object\n",
       "format               object\n",
       "unit_price          float64\n",
       "discount              int64\n",
       "quantity              int64\n",
       "yearmonth            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read `household.csv` in data_input folder\n",
    "household = __________\n",
    "\n",
    "# check `household` data types\n",
    "household._____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all columns are in the right data types, except for `purchase_time`. The correct data type for this column would have to be a `datetime`. In previous module, you've learned how you can use `.astype()` to adjust a data type for a column. In fact, pandas has a function to work with datetime object in particular.\n",
    "\n",
    "To convert a column `x` to a datetime, we would use:\n",
    "\n",
    "    `x = pd.to_datetime(x)`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id                   int64\n",
       "receipts_item_id             int64\n",
       "purchase_time       datetime64[ns]\n",
       "category                    object\n",
       "sub_category                object\n",
       "format                      object\n",
       "unit_price                 float64\n",
       "discount                     int64\n",
       "quantity                     int64\n",
       "yearmonth                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pd.to_datetime() to convert `purchase_time`\n",
    "_____ = pd.to_datetime(_____)\n",
    "    \n",
    "# check dtypes\n",
    "household._____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike using `astype()`, with `pd.to_datetime()` you are allowed to specify more arguments for the datetime conversion. Why it matters? Suppose we have a column which stores a daily sales data from end of January to the beginning of February:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30-01-2020\n",
       "1    31-01-2020\n",
       "2    01-02-2020\n",
       "3    02-02-2020\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = pd.Series(['30-01-2020', '31-01-2020', '01-02-2020','02-02-2020'])\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legal and cultural expectations for datetime format may vary between countries. In Indonesia for example, most people are used to storing dates in DMY order. Let's see what happen next when we convert our `date` to datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-01-30\n",
       "1   2020-01-31\n",
       "2   2020-01-02\n",
       "3   2020-02-02\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date.astype('datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look on the third observation; rather than representing February 1st as it suppose, the data converted to January 2nd. Thing to note here, for dates with multiple representations, `pandas` will infer it as a month first order by default.\n",
    "\n",
    "Using `pd.to_datetime`, you can specify your date formatting with parameters such as `format`* or `dayfirst`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-01-30\n",
       "1   2020-01-31\n",
       "2   2020-02-01\n",
       "3   2020-02-02\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.to_datetime(date, format='%d-%m-%Y')\n",
    "pd.to_datetime(date, dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*_Using Python's `datetime` module, `pandas` pass the date string to `.strptime()` and follows by what's called Python's strptime directives. The full list of directives can be found in this [Documentation](https://strftime.org/)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note** \n",
    "\n",
    "Beberapa metode untuk converting datetime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `data['column'].astype('datetime64')`\n",
    "2. `pd.to_datetime(data['column'])`: digunakan ketika ingin menambahkan pangaturan tambahan pada data waktu yang ingin di convert. Contoh parameter yang bisa ditambahkan:\n",
    "    - `dayfirst=True`: digunakan ketika format tanggal dimulai dengan tanggal (e.g.: `[11-01-2020,12-01-2020,1-01-2020]`)\n",
    "3. `pd.read_csv('data.csv', parse_dates=['column'])`: digunakan ketika kita sudah mengetahui kolom-kolom bernilai waktu pada file yang ingin kita baca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than `to_datetime`, `pandas` has a number of machineries to work with `datetime` objects. These are convenient for when we need to extract the `month`, or `year`, or `weekday_name` from `datetime`. Some common applications in business analysis include:\n",
    "\n",
    "- `household['purchase_time'].dt.month`\n",
    "- `household['purchase_time'].dt.month_name()`\n",
    "- `household['purchase_time'].dt.year`\n",
    "- `household['purchase_time'].dt.day`\n",
    "- `household['purchase_time'].dt.dayofweek`\n",
    "- `household['purchase_time'].dt.hour`\n",
    "- `household['purchase_time'].dt.day_name()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other functions that can be helpful in certain situations. Supposed we want to transform the existing `datetime` column into values of periods we can use the `.to_period` method:\n",
    "\n",
    "- `household['purchase_time'].dt.to_period('D')`\n",
    "- `household['purchase_time'].dt.to_period('W')`\n",
    "- `household['purchase_time'].dt.to_period('M')`\n",
    "- `household['purchase_time'].dt.to_period('Q')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:** Date time types  \n",
    "_Est. Time required: 20 minutes_\n",
    "\n",
    "1. In the following cell, start again by reading in the `household.csv` dataset. Drop `receipt_id` and `sub_category` columns as we won't use the columns for our analysis.  \n",
    "2. Make sure the `purchase_time` column has converted as a datetime object.\n",
    "3. Use `x.dt.day_name()`, assuming `x` is a datetime object to get the day of week. Assign this to a new column in your `household` Data Frame, name it `weekday`\n",
    "4. The `yearmonth` column stores the information of year and month of the `purchase_time`. Using `dt.to_period()`, how will you recreate the column if you needed the same information?\n",
    "5. Print the first 5 rows of your data to verify that your preprocessing steps are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips: In the cell above, start from:\n",
    "\n",
    "`household = pd.read_csv(\"data_input/household.csv\")`\n",
    "\n",
    "Inspect the first 5 rows of your data and pay close attention to the `weekday` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "\n",
    "## -- Solution code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus challenge:**  \n",
    "\n",
    "Suppose that the estimated shipping time will take around 2 days after the products being purchased. Create a new column, name it `shipdate_est` which stores the estimated shipping time of each transaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of `dtypes`, we see that there are three variables currently stored as `object` type where a `category` is more appropriate. This is a common diagnostic step, and one that you will employ in almost every data analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id                   int64\n",
       "receipts_item_id             int64\n",
       "purchase_time       datetime64[ns]\n",
       "category                    object\n",
       "sub_category                object\n",
       "format                      object\n",
       "unit_price                 float64\n",
       "discount                     int64\n",
       "quantity                     int64\n",
       "yearmonth                period[M]\n",
       "weekday                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "household.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what you have learned in the previous module about pandas data types, which columns do you think appropriate to be converted as `category`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional]: Alternative Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "receipt_id                   int64\n",
       "receipts_item_id             int64\n",
       "purchase_time       datetime64[ns]\n",
       "unit_price                 float64\n",
       "discount                     int64\n",
       "quantity                     int64\n",
       "yearmonth                period[M]\n",
       "weekday                   category\n",
       "category                  category\n",
       "sub_category              category\n",
       "format                    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = household.select_dtypes(exclude='object')\n",
    "data2 = household.select_dtypes(include='object').apply(pd.Series.astype, dtype='category')\n",
    "\n",
    "pd.concat([data1, data2], axis=1).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objectcols = household.select_dtypes(include='object')\n",
    "household[objectcols.columns] = objectcols.astype('category')\n",
    "household.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra notes on `category`: Why is it important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [pandas documentation](https://pandas.pydata.org/pandas-docs/version/0.15.1/categorical.html), the categorical data type is useful in the following cases:\n",
    "\n",
    "- A string variable consisting of only a few different values. Converting such a string variable to a categorical variable will save some memory, see [here](https://pandas.pydata.org/pandas-docs/version/0.15.1/categorical.html#categorical-memory).\n",
    "- The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order.\n",
    "- As a signal to other python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pay attention to `weekday`. The main different between `object` and `category` is that categorical object also storing information of how each observation should belong to certain group/categories. You can access the category by using `.cat.categories` as such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .cat.categories: access the categorical values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reorder the categories to follow the true day order by using `.cat.reorder_categories`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_order = ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "household['weekday'] =\\\n",
    "household['weekday'].cat.reorder_categories(day_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the order is important?**\n",
    "\n",
    "You might now wonder why should we order the level of our categorical data? Note that all analysis you'll go through on the categorical column will always refer to its `categories`; e.g. if you want to present a plot or a table as a result of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contingency Tables\n",
    "\n",
    "One of the simplest EDA toolkit is the frequency table (contingency tables) and cross-tabulation tables. It is highly familiar, convenient, and practical for a wide array of statistical tasks. The simplest form of a table is to display counts of a `categorical` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pandas`, each column of a `DataFrame` is a `Series`. To get the counts of each unique levels in a categorical column, we can use `.value_counts()`. The resulting object is a `Series` and in descending order so that the most frequent element is on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "household['sub_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and perform `.value_counts()` on the `format` column, adding either:\n",
    "\n",
    "- `sort=False` as a parameter to prevent any sorting of elements, or\n",
    "- `ascending=True` as a parameter to sort in ascending order instead\n",
    "\n",
    "How do you think each parameter works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# sort= True :(default): ________\n",
    "# sort= False: ________\n",
    "\n",
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascending=True : ________\n",
    "# ascending=False:(default) : ________\n",
    "\n",
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crosstab` is a very versatile solution to producing frequency tables on a `DataFrame` object. Its utility really goes further than that but we'll start with a simple use-case.\n",
    "\n",
    "Consider the following code: we use `pd.crosstab()` passing in the values to group by in the rows (`index`) and columns (`columns`) respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Detergent</th>\n",
       "      <td>36000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rice</th>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sugar</th>\n",
       "      <td>24000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0         count\n",
       "sub_category       \n",
       "Detergent     36000\n",
       "Rice          12000\n",
       "Sugar         24000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index = _____,\n",
    "            columns = _____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize that in the code above, we're setting the row (index) to be `sub_category` and the function will by default compute a frequency table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we set the values to be normalized over each columns, and this will divide each values in place over the sum of all values. This is equivalent to a manual calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "catego = pd.crosstab(index=household['sub_category'], columns=\"count\")\n",
    "#catego / catego.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the same `crosstab` method to compute a cross-tabulation of two factors. In the following cell, the `index` references the sub-category column while the `columns` references the format column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = _____,\n",
    "            columns = _____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intuitive in a way: We use `crosstab()` which, we recall, computes the count and we pass in `index` and `columns` which correspond to the row and column respectively.\n",
    "\n",
    "When we add `margins=True` to our method call, then an extra row and column of margins (subtotals) will be included in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:**\n",
    "\n",
    "Use `pd.crosstab()` to answer following business question:\n",
    "\n",
    "- Cobalah untuk menganalisis jumlah transaksi perbulan di setiap `format` market penjualan. Di bulan berapakah jumlah transaksi terendah terjadi untuk setiap `format` market? \n",
    "\n",
    "*Tips:*\n",
    "\n",
    "- Anda dapat menggunakan `.idxmin()` pada tabel yang Anda menghasilkan untuk mendapatkan indeks terendahnya.\n",
    "\n",
    "---\n",
    "\n",
    "*Bonus Challenge*\n",
    "\n",
    "- Dalam rangka menaikkan jumlah transaksi di `minimarket`, perusahaan berencana untuk mengadakan *flash-sale* di jam dengan transaksi terendah. Gunakan data transaksi di `minimarket` pada bulan terendahnya (Recall conditional subsetting!), dan buatlah sebuah kolom baru yang menyimpan informasi jam terjadinya transaksi. Di jam berapakah *flash-sale* tersebut sebaiknya diadakan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "\n",
    "## -- Solution code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want an extra challenge, try and modify your code above to include a `normalize` parameter. \n",
    "\n",
    "`normalize` accepts a boolean value, or one of `all`, `index` or `columns`. Since we want it to normalize across each row, we will set this parameter to the value of `index`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Table\n",
    "\n",
    "In the following section, we will introduce another parameter to perform aggregation on our table. The `aggfunc` parameter when present, required the `values` parameter to be specified as well. `values` is the values to aggregate according to the factors in our index and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check**: Cross tabulation  \n",
    "\n",
    "Create a cross-tab using `sub_category` as the index (row) and `format` as the column. Fill the values with the median of `unit_price` across each row and column. Add a subtotal to both the row and column by setting `margins=True`.\n",
    "\n",
    "1. On average, Sugar is cheapest at...?\n",
    "2. On average, Detergent is most expensive at...?\n",
    "\n",
    "Create a new cell for your code and answer the questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "\n",
    "## -- Solution code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-dimensional Tables\n",
    "\n",
    "If we need to inspect our data in higher resolution, we can create cross-tabulation using more than one factor. This allows us to yield insights on a more granular level yet have our output remain relatively compact and structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <th colspan=\"3\" halign=\"left\">hypermarket</th>\n",
       "      <th colspan=\"3\" halign=\"left\">minimarket</th>\n",
       "      <th colspan=\"3\" halign=\"left\">supermarket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_category</th>\n",
       "      <th>Detergent</th>\n",
       "      <th>Rice</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>Detergent</th>\n",
       "      <th>Rice</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>Detergent</th>\n",
       "      <th>Rice</th>\n",
       "      <th>Sugar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearmonth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10</th>\n",
       "      <td>17,400.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,800.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,925.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11</th>\n",
       "      <td>16,770.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "      <td>16,800.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,500.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12</th>\n",
       "      <td>17,500.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,000.0</td>\n",
       "      <td>16,600.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,600.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01</th>\n",
       "      <td>16,800.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,275.0</td>\n",
       "      <td>16,200.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,700.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02</th>\n",
       "      <td>17,500.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>11,990.0</td>\n",
       "      <td>17,000.0</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,200.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03</th>\n",
       "      <td>16,900.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,000.0</td>\n",
       "      <td>16,300.0</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>15,680.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04</th>\n",
       "      <td>16,815.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>11,990.0</td>\n",
       "      <td>16,800.0</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>15,700.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05</th>\n",
       "      <td>16,950.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,000.0</td>\n",
       "      <td>16,800.0</td>\n",
       "      <td>63,000.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,700.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06</th>\n",
       "      <td>16,550.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,300.0</td>\n",
       "      <td>17,300.0</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,700.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07</th>\n",
       "      <td>16,550.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,325.0</td>\n",
       "      <td>16,800.0</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,600.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08</th>\n",
       "      <td>16,839.0</td>\n",
       "      <td>62,600.0</td>\n",
       "      <td>12,000.0</td>\n",
       "      <td>17,500.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>17,000.0</td>\n",
       "      <td>64,000.0</td>\n",
       "      <td>12,300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09</th>\n",
       "      <td>16,720.0</td>\n",
       "      <td>60,000.0</td>\n",
       "      <td>11,900.0</td>\n",
       "      <td>16,900.0</td>\n",
       "      <td>62,500.0</td>\n",
       "      <td>12,500.0</td>\n",
       "      <td>16,990.0</td>\n",
       "      <td>62,550.0</td>\n",
       "      <td>12,300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "format       hypermarket                   minimarket                    \\\n",
       "sub_category   Detergent     Rice    Sugar  Detergent     Rice    Sugar   \n",
       "yearmonth                                                                 \n",
       "2017-10         17,400.0 64,000.0 12,500.0   16,800.0 62,500.0 12,500.0   \n",
       "2017-11         16,770.0 64,000.0 12,400.0   16,800.0 62,500.0 12,500.0   \n",
       "2017-12         17,500.0 64,000.0 12,000.0   16,600.0 62,500.0 12,500.0   \n",
       "2018-01         16,800.0 64,000.0 12,275.0   16,200.0 62,500.0 12,500.0   \n",
       "2018-02         17,500.0 64,000.0 11,990.0   17,000.0 63,500.0 12,500.0   \n",
       "2018-03         16,900.0 64,000.0 12,000.0   16,300.0 63,500.0 12,500.0   \n",
       "2018-04         16,815.0 64,000.0 11,990.0   16,800.0 63,500.0 12,500.0   \n",
       "2018-05         16,950.0 64,000.0 12,000.0   16,800.0 63,000.0 12,500.0   \n",
       "2018-06         16,550.0 64,000.0 12,300.0   17,300.0 63,500.0 12,500.0   \n",
       "2018-07         16,550.0 64,000.0 12,325.0   16,800.0 63,500.0 12,500.0   \n",
       "2018-08         16,839.0 62,600.0 12,000.0   17,500.0 62,500.0 12,500.0   \n",
       "2018-09         16,720.0 60,000.0 11,900.0   16,900.0 62,500.0 12,500.0   \n",
       "\n",
       "format       supermarket                    \n",
       "sub_category   Detergent     Rice    Sugar  \n",
       "yearmonth                                   \n",
       "2017-10         16,925.0 64,000.0 12,500.0  \n",
       "2017-11         16,500.0 64,000.0 12,400.0  \n",
       "2017-12         16,600.0 64,000.0 12,400.0  \n",
       "2018-01         16,700.0 64,000.0 12,400.0  \n",
       "2018-02         16,200.0 64,000.0 12,290.0  \n",
       "2018-03         15,680.0 64,000.0 12,400.0  \n",
       "2018-04         15,700.0 64,000.0 12,400.0  \n",
       "2018-05         16,700.0 64,000.0 12,400.0  \n",
       "2018-06         16,700.0 64,000.0 12,400.0  \n",
       "2018-07         16,600.0 64,000.0 12,300.0  \n",
       "2018-08         17,000.0 64,000.0 12,300.0  \n",
       "2018-09         16,990.0 62,550.0 12,300.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=household['yearmonth'], \n",
    "            columns=____________, \n",
    "            values=household['unit_price'],\n",
    "            aggfunc='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pandas` we call a higher-dimensional tables as Multi-Index Dataframe. We are going to dive deeper into the structure of the object on the the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "If our data is already in a `DataFrame` format, using `pd.pivot_table` can sometimes be more convenient compared to a `pd.crosstab`. \n",
    "\n",
    "Fortunately, much of the parameters in a `pivot_table()` function is the same as `pd.crosstab()`. The noticable difference is the use of an additional `data` parameter, which allow us to specify the `DataFrame` that is used to construct the pivot table.\n",
    "\n",
    "We create a `pivot_table` by passing in the following:\n",
    "- `data`: our `DataFrame`\n",
    "- `index`: the column to be used as rows\n",
    "- `columns`: the column to be used as columns\n",
    "- `values`: the values used to fill in the table\n",
    "- `aggfunc`: the aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values and Duplicates\n",
    "\n",
    "During the data exploration and preparation phase, it is likely we come across some problematic details in our data. This could be the value of _-1_ for the _age_ column, a value of _blank_ for the _customer segment_ column, or a value of _None_ for the _loan duration_ column. All of these are examples of \"untidy\" data, which is rather common depending on the data collection and recording process in a company.\n",
    "\n",
    "In `pandas`, we use `NaN` (not a number) to denote missing data; The equivalent for datetime is `NaT` but both are essentially compatible with each other. From the docs:\n",
    "> The choice of using `NaN` internally to denote missing data was largely for simplicity and performance reasons. We are hopeful that NumPy will soon be able to provide a native NA type solution (similar to R) performant enough to be used in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saat membaca data, jika kolom tidak benar2 kosong, tapi diisi dgn nilai tertentu (Contoh: \"Missing\",\" \", \"-\"), gunakan parameter `na_values()` untuk mendefinisikannya:\n",
    "\n",
    "```\n",
    "household = pd.read_csv(\"data.csv\",na_values = [\"Missing\",\" \",\"-\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"na.csv\",na_values=[\"-\",\" \",\"Missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "household = pd.read_csv(\"data_input/sample_household.csv\",\n",
    "                        index_col = 'receipts_item_id',\n",
    "                       parse_dates=['purchase_time'])\n",
    "household.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the output that between row 3 to 8 there are at least a few rows with missing data. We can use `isna()` and `notna()` to detect missing values. An example code is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mengembalikan jumlah baris yang kosong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengembalikan jumlah baris yang TIDAK kosong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of using the `.isna()` method is to combine it with the subsetting methods we've learned in previous lessons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghasilkan data yg memiliki nilai NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# menghasilkan data yg memiliki nilai NA pada kolom weekday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and use `notna()` to extract all the rows where `weekday` column is not missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghasilkan data yg TIDAK memiliki nilai NA pada kolom weekday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Treatment\n",
    "\n",
    "Once you've identified the missing values, there are 3 common ways to deal with it:\n",
    "\n",
    "- Use `dropna` with a reasonable threshold to remove any rows that contain too little values rendering it unhelpful to your analysis\n",
    "- Replace the missing values with a central value (mean or median)\n",
    "- Imputation through a predictive model\n",
    "\n",
    "#### NA Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are certain that the rows with `NA`s can be safely dropped, we can use `dropna()`, optionally specifying a threshold. By default, this method drops the row if any NA value is present (`how='any'`), but it can be set to do this only when all values are NA in that row (`how='all'`).\n",
    "\n",
    "```\n",
    "    # drops row if all values are NA\n",
    "    household.dropna(how='all')\n",
    "    \n",
    "    # drops row if it doesn't have at least 5 non-NA values\n",
    "    household.dropna(thresh=5) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NA Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common methods when working with missing values are demonstrated in the following section. We make a copy of the NA-included DataFrame, and name it `household2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "household2 = household.copy()\n",
    "household2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the technique is demonstrably repetitive or even verbose. This is done to give us an idea of all the different options we can pick from. \n",
    "\n",
    "You may observe, for example that the two lines of code are functionally identical:\n",
    "- `.fillna(0)`\n",
    "- `.replace(np.nan, 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>category</th>\n",
       "      <th>format</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>quantity</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receipts_item_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32000000</th>\n",
       "      <td>2018-07-17 18:05:00</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000001</th>\n",
       "      <td>2018-07-17 18:05:00</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32030785</th>\n",
       "      <td>2018-07-17 18:05:00</td>\n",
       "      <td>Rice</td>\n",
       "      <td>minimarket</td>\n",
       "      <td>63,500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000002</th>\n",
       "      <td>2018-07-22 21:19:00</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000003</th>\n",
       "      <td>2018-07-22 21:19:00</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        purchase_time category      format  unit_price  \\\n",
       "receipts_item_id                                                         \n",
       "32000000          2018-07-17 18:05:00  Missing     Missing         0.0   \n",
       "32000001          2018-07-17 18:05:00  Missing     Missing         0.0   \n",
       "32030785          2018-07-17 18:05:00     Rice  minimarket    63,500.0   \n",
       "32000002          2018-07-22 21:19:00  Missing     Missing         0.0   \n",
       "32000003          2018-07-22 21:19:00  Missing     Missing         0.0   \n",
       "\n",
       "                 discount  quantity  weekday  \n",
       "receipts_item_id                              \n",
       "32000000          Missing      -1.0      NaN  \n",
       "32000001          Missing      -1.0      NaN  \n",
       "32030785              0.0       1.0  Tuesday  \n",
       "32000002          Missing      -1.0      NaN  \n",
       "32000003          Missing      -1.0      NaN  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert NA categories to 'Missing'\n",
    "household2[['category', 'format','discount']] = household2[['category', 'format','discount']].fillna('Missing')\n",
    "\n",
    "# convert NA unit_price to 0\n",
    "household2.unit_price = household2.unit_price.fillna(0)\n",
    "\n",
    "# convert NA purchase_time with 'bfill'\n",
    "household2.purchase_time = pd.to_datetime(household2.purchase_time)\n",
    "household2.purchase_time = household2.fillna(method='bfill')\n",
    "\n",
    "# convert NA quantity with -1\n",
    "household2.quantity = household2.quantity.fillna(-1)\n",
    "\n",
    "household2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether our data has any duplicates, we can use `.duplicated()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have data where duplicated observations are recorded, we can use `.drop_duplicates()` specifying whether the first occurence or the last should be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 7)\n",
      "(16, 7)\n"
     ]
    }
   ],
   "source": [
    "print(household2.shape)\n",
    "print(household2.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:**   \n",
    "\n",
    "Duplicates may mean a different thing from a data point-of-view and a business analyst's point-of-view. You want to be extra careful about whether the duplicates is an intended characteristic of your data, or whether it poses a violation to the business logic. \n",
    "\n",
    "Would you drop the \"duplicated\" values:\n",
    "\n",
    "   - a. A medical center collects anonymized heart rate monitoring data from patients. It has duplicate observations collected across a span of 3 months.\n",
    "   - b. An insurance company uses machine learning to deliver dynamic pricing to its customers. Each row contains the customer's name, occupation / profession and historical health data. It has duplicate observations collected across a span of 3 months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between `crosstab` and `pivot_table` is that `crosstab` uses `len` (or `count`) as the default aggregation function while `pivot_table` using the mean. Copy the code from the cell above and make a change: use `sum` as the aggregation function instead: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_newton",
   "language": "python",
   "name": "da_newton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "787px",
    "left": "108px",
    "top": "59px",
    "width": "190.795px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
